# Example environment file for local development
# Copy to .env and replace placeholders with your own values.
# DO NOT commit real secrets to version control.

# LLM providers
OPENAI_API_KEY=__REPLACE_WITH_YOUR_KEY__
OPENAI_MODEL=gpt-4o-mini
OPENAI_BASE_URL=https://api.openai.com/v1

# Optional local model host (e.g., Ollama or on-prem deployment)
# Set OPNXT_MODEL_PROVIDER=local to force the router to use this block.
# Local LLM provider
OPNXT_MODEL_PROVIDER=local
OPNXT_ENABLE_LOCAL_PROVIDER=1
LOCAL_BASE_URL=http://192.168.37.149:11434
LOCAL_MODEL=gpt-oss:120b
LOCAL_MODEL_FALLBACKS=mixtral:8x22b,qwen2.5:72b-instruct,llama3.1:70b,mixtral:8x22b,mistral:latest,llama3.2:latest
# LOCAL_API_KEY= # only if your local server enforces auth

# --- opnxt-stream ---
# Streaming & resilience toggles
OPNXT_DISABLE_LOCAL_LLM=0
OPNXT_LLM_BREAKER_THRESHOLD=2
OPNXT_LLM_BREAKER_COOLDOWN=120
OPNXT_LLM_BREAKER_PROBE_INTERVAL=15
OPNXT_LLM_CONNECT_TIMEOUT=3
OPNXT_LLM_READ_TIMEOUT=15
OPNXT_ENABLE_STREAMING=1
OPNXT_STREAM_HEARTBEAT_SECONDS=5
OPNXT_STREAM_POLL_SECONDS=0.5

# Auth / JWT (dev defaults exist; prefer to set your own)
JWT_SECRET=change-me-in-dev
JWT_EXPIRES_MIN=60

# Public mode (true allows anonymous guest contributor for MVP/demo). For tests/CI set to false.
OPNXT_PUBLIC_MODE=false

# Optional: toggle CI behavior
CI=
GITHUB_ACTIONS=

DB_MODE=memory
OPNXT_REPO_IMPL=memory
OPNXT_DOC_STORE_IMPL=memory
OPNXT_CHAT_STORE_IMPL=memory
MONGO_URL=mongodb://localhost:27017
MONGO_DB=opnxt
REDIS_URL=redis://localhost:6379/0
